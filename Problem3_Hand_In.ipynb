{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Problem3_Agos_Hand_In.ipynb","provenance":[{"file_id":"1jH2Fx4BQ1cXnoqym7CE4WyszABoTOSzF","timestamp":1614093896507},{"file_id":"1mkFq3_-SUzGm8rVV1kSY_ovkbcjWxWi-","timestamp":1614025244498}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"DMaA_02oZHlU"},"source":["# **##################### Problem 3 #####################**\n","\n","In the following, you will load some data, i.e., simulation results from training a 500-neurons RNN to mimic target functions similar to calcium fluorescence signals collected from a population of active neurons in the prefrontal cortex of a mammalian nervous system. \n","\n","Your task is to analyze these data using the following a recipe, and plot the results.\n"]},{"cell_type":"markdown","metadata":{"id":"ztVfPVQ7uAX8"},"source":["As a first step, let's load and open the data. Please follow the instructions carefully.\n","\n","1. Download the data file ('pset_p3.mat') from here:\n","https://drive.google.com/file/d/1imOA-ipWWrv-CXobZ2rMUiiUxN0Hsc8B/view?usp=sharing\n","\n","2. Upload the dataset 'pset_p3.mat' to your personal Google Drive. Please just upload it to the top level, i.e. not within any folder.\n","\n","3. Run the piece of code just below. \n","The code connects to your Google Drive via this part here:\n","```\n","from google.colab import drive\n","drive.mount('/content/drive')\n","```\n","It will prompt you to open a new tab, sign in to your google account, and copy a password. You'll have to enter the password below the prompt. \n","\n","Note: if you correctly placed the file in the main folder of your Drive, you should not get any error. However, in case you get errors, you can try to look for the file manually: there's a Files button in the left column of this colab, where you can browse through your Google Drive. \n"]},{"cell_type":"code","metadata":{"id":"cHVYxwaaG0PI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614095531287,"user_tz":300,"elapsed":796,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}},"outputId":"51b29b6c-0da7-4822-d306-8fae98d9384a"},"source":["### Import libraries\n","\n","import time\n","import numpy as np\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Some more definitions for plotting\n","\n","colors = np.array([[0.1254902 , 0.29019608, 0.52941176],\n","       [0.80784314, 0.36078431, 0.        ],\n","       [0.30588235, 0.60392157, 0.02352941],\n","       [0.64313725, 0.        , 0.        ]])\n","\n","figsize = (8, 3)\n","\n","### Import data file\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","data_dir = \"drive/MyDrive/pset_p3.mat\"\n","try:\n","    import hdf5storage\n","except:\n","    !pip install hdf5storage\n","    import hdf5storage\n","p3_info = hdf5storage.loadmat(data_dir)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HLh8YoYI0YgC"},"source":["If you manage to correctly load the file, then the file contains:\n","* Data in a neurons x time matrix used as target functions to train the above; \n","* Initial random and trained recurrent interaction matrices (both NxN sized).  \n","* Firing rates of the trained RNN in a neurons x time matrix; \n","* firing rates of the same RNN before training in a neurons x time matrix.\n","\n","Specifically, the content of the file is as follows:\n","\n"," ```\n","N     # N neurons\n","g     # variance of the initial weights\n","\n","# Network activity of untrained and trained network\n","t     # time vector \n","R0    # Rates of untrained network  (N x time)\n","R     # Rates of trained network  (N x time)\n","\n","# Target activitiy and corresponding times\n","target_t            # time vector for targets\n","target_activity     # targets\n","\n","J0    # Initial random weight matrix (NxN)\n","J     # Trained weight matrix (NxN)\n","```\n","\n","In the following piece of code, we extract those quantities from the file.\n"]},{"cell_type":"code","metadata":{"id":"rSa3cSHt0YHw","executionInfo":{"status":"ok","timestamp":1614095531288,"user_tz":300,"elapsed":785,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["#Load variables into name space\n","\n","# Number of neurons, g\n","N = p3_info[\"N\"]\n","g = p3_info[\"g\"]\n","# Activity before and after training\n","t = p3_info[\"t\"]\n","R0 = p3_info[\"R0\"]\n","R = p3_info[\"R\"]\n","# Training targets\n","target_t = p3_info[\"target_t\"]\n","target_R = p3_info[\"target_activity\"]\n","# Weight matrices\n","J0 = p3_info[\"J0\"]\n","J = p3_info[\"J\"]\n","\n","# Join activity matrices\n","lbls = [\"Before training\", \"After training\", \"Target\"]\n","Rs = [R0, R, target_R]"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y05UARzEZHlk"},"source":["## Dynamics\n","\n","**a)** Plot the firing rate of 3 randomly selected units in the RNN super-imposed on their respective target functions. \n"]},{"cell_type":"markdown","metadata":{"id":"Cj35u98oNvE_"},"source":["**SOLUTION:**"]},{"cell_type":"code","metadata":{"id":"DrdmVEFojIDI","executionInfo":{"status":"ok","timestamp":1614095531288,"user_tz":300,"elapsed":779,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":[""],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"recfYsjcKSaX"},"source":["The target looks like a moving Gaussian bump, with higher neuron indices corresponding to later times. \n","\n","The trained activity tracks the target, even though in a noisy manner. \n","\n","For completeness, we visualize activity for all neurons in the target, initial and trained dataset."]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"aUc8st_GZHlw","executionInfo":{"status":"ok","timestamp":1614095531288,"user_tz":300,"elapsed":773,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Plot activity for all neurons before/after training and target\n","\n"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M1a97a9n3W4c"},"source":["**b)** You figured out how to do PCA in the first hands-on lab session. Can you generate the same state space diagram for the outputs of the trained network? Compared to the state space picture for the untrained RNN, how has training changed the picture? How about when compared to the “target data”?"]},{"cell_type":"markdown","metadata":{"id":"yro_9ZL_4HRY"},"source":["**SOLUTION:** We use the scikit learn routine. For each dataset (R_i in Rs), we create a pca object, run the fit, and project the dataset on the principal components. "]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"EukoGKRgZHln","executionInfo":{"status":"ok","timestamp":1614095531289,"user_tz":300,"elapsed":768,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Compute PCA for all three matrices\n","\n","# Number of components to keep\n","n_comp = 50\n","\n"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91_vP0sPMpPf"},"source":["We now plot projections of activity of PCs."]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"W8DVRzXEZHlo","executionInfo":{"status":"ok","timestamp":1614095545450,"user_tz":300,"elapsed":192,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Plot activity in state space (projected separately on each datasets' PCs)\n","\n","# 2D\n","\n"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_wsGLlBG4hka"},"source":["It turns out that this doesn't reveal so much. All trajectories have somewhat circular elements, the untrained one also has some initial transient (the straight line). But we projected on different axes for each plot, so there's not so much to compare. \n","\n","Let's project all the trajectories on only one set of PCs, e.g. the one of the trained network, and plot this:"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"OysBgG4sZHlp","executionInfo":{"status":"ok","timestamp":1614095549240,"user_tz":300,"elapsed":335,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Plot activity in state space (projected on trained dataset's PCs)\n","\n","# Project all one the axes computed for the trained networks. \n","\n","# hint: use \n","\n","# pca_proj.transform(R_i.T)\n","\n"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHND92hd5FmD"},"source":["This gives some more insight. First, the initial activity, despite being of similar magnitude, has only a small and very unstructured projection on the PCs of the trained network. But the trained network and the target evolve in a very similar manner -- the projection of the target is about the same size as when projected on its own PCs. (One could also quantify this by computing the projection of two sets of eigenvectors)."]},{"cell_type":"markdown","metadata":{"id":"7cN7W9xr6Du4"},"source":["**c)** Can you compute, and plot, how many principal components of the RNN capture 90% of the total variance of the activity? This should be a graph plotting percent variance, between 0 and 100, on the y-axis as a function of PC # on the x-axis. \n"]},{"cell_type":"markdown","metadata":{"id":"GvH8vVpANVLy"},"source":["**SOLUTION:**"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"G47thcswZHlr","executionInfo":{"status":"aborted","timestamp":1614095531962,"user_tz":300,"elapsed":1408,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Plot explained variance\n","\n","fig = plt.figure(figsize=figsize)\n","ax = fig.subplots(1, 1)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcIMq0pG6HoM"},"source":["The plot shows that the trained network (as well as the target) has a substantially reduced dimensionality compared with the untrained network. 10 PCs cover 90% of the variance. "]},{"cell_type":"markdown","metadata":{"id":"GEa9lKVsZHlt"},"source":["## Connectivity inferred from the RNN\n","\n","**a)** How has training changed the recurrent interactions? One way to quantify that is to look at the overall magnitude of the change, i.e., the norm of the difference between the untrained random matrix and the trained matrix inferred from the RNN fit to data. Compare this to the norm of the difference between any other two random matrices with  same size and variance as the untrained matrix. "]},{"cell_type":"markdown","metadata":{"id":"hejZAdGA7aFk"},"source":["**SOLUTION:** The norm has increased dramatically, almost 7-fold. "]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"TmfZn6vqZHlu","executionInfo":{"status":"aborted","timestamp":1614095531963,"user_tz":300,"elapsed":1399,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Compare norm difference between untrained and trained weights\n","\n","# Compare with norm diff of two random matrices\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYyCTLum7qKn"},"source":["**b)** Can you make a histogram of the entries in the trained recurrent matrix and compare it to the histogram of the elements in the untrained, randomly initialized recurrent matrix? The y-axis should be the log-frequency of the binned elements, and the x-axis should be the binned weight values. \n","\n","**c)**  Can you replot the same histograms as in (b) so the y-axes reflect logarithmic probbility density?"]},{"cell_type":"markdown","metadata":{"id":"88EFN8wSOl2d"},"source":["**SOLUTION:**"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"EZlquYHnZHlu","executionInfo":{"status":"aborted","timestamp":1614095531964,"user_tz":300,"elapsed":1389,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Histogram of entries\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLo29jQJ8J1p"},"source":["The histograms reveal that the matrix changed dramatically: many weights are much larger in magnitude. However, the final distribution seems to be the sum of two distribution, where one is very similar to the distribution of untrained weights. \n","\n","To obtain a better idea what's going on there, it's worthwhile plotting the weight matrices themselves. "]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"vFzz0_kcZHlw","executionInfo":{"status":"aborted","timestamp":1614095531964,"user_tz":300,"elapsed":1379,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Plot matrices\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zn13JDXL-1FW"},"source":["This looks strange! Only a small number of columns seem to be much larger than the rest. The reason is that actually only 50 units in the network were trained. For a deeper analysis of the network and its dynamics, it would probably make sense to separate these trained ones from the rest..."]},{"cell_type":"markdown","metadata":{"id":"LdQ1cCIw_UwO"},"source":["**d)** Have your code spit out the first 4 moments of the trained and random matrices, i.e., mean, variance, kurtosis, and skewness. Which of these modes has changed the most as a function of training to match the data, and what does it mean? \n"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"ClduDSgFZHlv","executionInfo":{"status":"aborted","timestamp":1614095531964,"user_tz":300,"elapsed":1370,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# First 4 moments of recurrent matrices\n","from scipy.stats import skew, kurtosis\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JslKguQ9_k-r"},"source":["The trained matrix is changed drastically on all moments. Relatively, the kurtosis changed most."]},{"cell_type":"markdown","metadata":{"id":"3dflKFNWBb2q"},"source":["**e)** Now that you have some intuition for the spectral modes of interaction matrices, as in Problem 1 **e**, can you plot the eigenvalues of the trained matrix? How has the distribution changed relative to where it started, i.e., that of the eigenvalue spectrum of the randomly initialized matrix? What do you think the implications of this change are, at the level of the dynamics? \n"]},{"cell_type":"markdown","metadata":{"id":"uiuPBzGsPjJ7"},"source":["**SOLUTION:**"]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"6-yUp-8WZHlv","executionInfo":{"status":"aborted","timestamp":1614095531965,"user_tz":300,"elapsed":1358,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":["# Plot eigenvalue spectrum\n","fig = plt.figure(figsize=(10, 4))\n","axes = fig.subplots(1, 2)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nXQhzj3uA3mr"},"source":["The eigenvalues are drastically increased, and the distribution is less uniform. The latter points to dynamics that unfold in a lower-dimensional space, in agreement with the picture we obtained from PCA. "]},{"cell_type":"markdown","metadata":{"id":"IkBYDJEdQ0xh"},"source":["\n"]},{"cell_type":"code","metadata":{"id":"gfQur6ZgXztq","executionInfo":{"status":"aborted","timestamp":1614095531965,"user_tz":300,"elapsed":1349,"user":{"displayName":"Agostina Palmigiano","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyXPCe1nToo7fPDmRClebbx4QYmd_jXvAEpiImkQ=s64","userId":"18363031759470736884"}}},"source":[""],"execution_count":null,"outputs":[]}]}